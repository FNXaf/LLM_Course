{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1DTlgfUmuG2qtMB5KZavM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FNXaf/LLM_Course/blob/main/day1_session1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Foundation of LLM and Introduction to Transformer Model\n"
      ],
      "metadata": {
        "id": "OH1kQ70rXOFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipelining in Transformer Model"
      ],
      "metadata": {
        "id": "W2jpALn0Xo6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "GB5faGWFXwNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Analysis using Transformer Model"
      ],
      "metadata": {
        "id": "4tioOGxUYJ6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "    \"I've been waiting for a HuggingFace course of my Life \",\n",
        "    \"I hate this so much!\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YtAQnIoiYQJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Generator Model (like chatgpt)"
      ],
      "metadata": {
        "id": "4U0qxmXvaQla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "generator (\"once upon a time in Nationa College of Engineering\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRcmvRv7ZEOD",
        "outputId": "adc6cd24-f23e-4d03-c4e3-a3aa9fd4f7e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'once upon a time in Nationa College of Engineering, where I was working on a project for a project of ours. It was a project of ours, and I wanted the name of the project to be a thing of the past, so I created a name for it. I\\'m an engineer, and I\\'m a project manager. I\\'m a project manager. We\\'re very different. We\\'re not going to become one. We\\'re working together. It\\'s a lot like what I\\'m doing now, but we\\'re not going to be like that.\\n\\nJULIE: You\\'re right. It\\'s a lot like what you did in the studio. When you come out, you\\'re like, \"I\\'m going to be like this person, and I\\'m going to be like this person in this project. I\\'m going to be like this person in this project.\" [laughter] So I did a lot of other things in the studio. I\\'m not going to talk about what happened in the studio. I\\'m going to talk about the stuff that we did. We were going to do a lot of other things.\\n\\nJULIE: So when you did the first project, we did it a couple of times. Then you got to do the second project. Did you do it too much'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##QnA Model"
      ],
      "metadata": {
        "id": "hHCh977qa5x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa= pipeline(\"question-answering\")\n",
        "qa(\n",
        "    question= \"where do i work?\",\n",
        "    context=\"once upon a time in national college of engineering\",\n",
        ")"
      ],
      "metadata": {
        "id": "-og_X-rPa9DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Language translator"
      ],
      "metadata": {
        "id": "GpWnkNAwbuAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation_en_to_de\")\n",
        "translator (\"I love S\")"
      ],
      "metadata": {
        "id": "ls66P4XZbyLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NER Model"
      ],
      "metadata": {
        "id": "FxzMXvJAc0L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "ner(\"I am Samriddhi and I live in Bhaktapur,Nepal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6rOS-NOc3MU",
        "outputId": "1d9f4922-a9e2-4fe7-f895-c926de0974a6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER',\n",
              "  'score': np.float32(0.9990878),\n",
              "  'index': 3,\n",
              "  'word': 'Sam',\n",
              "  'start': 5,\n",
              "  'end': 8},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.98565847),\n",
              "  'index': 4,\n",
              "  'word': '##rid',\n",
              "  'start': 8,\n",
              "  'end': 11},\n",
              " {'entity': 'I-PER',\n",
              "  'score': np.float32(0.9941256),\n",
              "  'index': 5,\n",
              "  'word': '##dhi',\n",
              "  'start': 11,\n",
              "  'end': 14},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.99865013),\n",
              "  'index': 10,\n",
              "  'word': 'B',\n",
              "  'start': 29,\n",
              "  'end': 30},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.9954529),\n",
              "  'index': 11,\n",
              "  'word': '##hak',\n",
              "  'start': 30,\n",
              "  'end': 33},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.88808805),\n",
              "  'index': 12,\n",
              "  'word': '##ta',\n",
              "  'start': 33,\n",
              "  'end': 35},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.9968195),\n",
              "  'index': 13,\n",
              "  'word': '##pur',\n",
              "  'start': 35,\n",
              "  'end': 38},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': np.float32(0.99960274),\n",
              "  'index': 15,\n",
              "  'word': 'Nepal',\n",
              "  'start': 39,\n",
              "  'end': 44}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}